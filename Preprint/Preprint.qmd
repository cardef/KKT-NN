---
title: KKT-Informed Neural Network
subtitle: A Parallel Solver for Parametric Convex Optimization Problem
author: 
    name: Carmine Delle Femine
    email: cdellefemine@vicomtech.org
    affiliation:
        name: Vicomtech Foundation
        department: Department of Data Intelligence for Energy and Industrial Processes
        address: Paseo Mikeletegi 57
        city: Donostia-San Sebastián
        region: Guipuzkoa
        state: Spain
        postal-code: 20009
        isni: 0000000460222780
        ror: https://ror.org/0023sah13
keywords:
    - Optimization
format:
    arxiv-pdf:
        keep-tex: true
        include-in-header:
            - text: |
                \usepackage{amsmath}
                \usepackage{tikz}
abstract: |
  This is the abstract  
---

# Introduction

# Background

Consider a parametric convex optimization problem in the standard form:

$$
\begin{aligned}
\min_{x \in \mathcal{D} \subseteq\mathbb{R}^n} \quad &f(x, {\theta})\\
\textrm{s.t.} \quad & g_i(x, \theta) \leq 0 \quad i = 1, \dots, m \\
& A(\theta) x - b(\theta) = 0
\end{aligned}
$$

where $x \in \mathcal{D} \subseteq\mathbb{R}^n$ is the optimization variable; $\theta \in \mathcal{D}_\theta \subseteq \mathbb{R}^k$ are the parameters defining the problem; $f: \mathcal{D}_f \subseteq\mathbb{R}^n \times \mathbb{R}^k \to \mathbb{R}$ is the convex cost function; $g_i: \mathcal{D}_{g_i} \subseteq\mathbb{R}^n \times \mathbb{R}^k \to \mathbb{R}$ are the convex inequality constraints, $A: \mathcal{D}_\theta \to \mathbb{R}^{p \times n}$ and $b: \mathcal{D}_\theta \to \mathbb{R}^{p}$ defines the affine equality constraints and $\mathcal{D} = \bigcap_{i=1}^{m} \mathcal{D}_{g_i} \cap \mathcal{D}_{f}$ is the domain of the optimization problem.

Assume differentiable cost and constraints functions and that $g_i$ satisfies Slater's condition. Given a set of parameters $\theta$, $x^* \in \mathcal{D}$ is optimal if and only if there are $\lambda^*$ and $\nu^*$ that, with $x^*$, satisy the Karush-Kuhn-Tucker conditions (KKT):

```{=tex}
\begin{align}
    A(\theta) x^* - b(\theta) = 0&\\
    g_i(x^*, \theta) \leq 0& \quad i=1,\dots, m\\
    \lambda_i^* \geq 0& \quad i=1,\dots, m\\
    \lambda_i^* g_i(x^*, \theta) = 0& \quad i=1,\dots, m\\
    \nabla_{x^*} f(x^*, \theta) + \sum\nolimits_{i=1}^m \lambda^*_i\nabla_{x^*} g_i(x^*, \theta) + A^T\nu^* = 0 &
\end{align}
```
# Proposed method

KKT-Informed Neural Network (KINN) builds upon the principles of Physics-Informed Neural Networks (PINNs), incorporating mathematical conditions of the Karush-Kuhn-Tucker (KKT) conditions directly into the neural architecture. This integration facilitates a disciplined learning process where the network not only predicts optimization variables but also ensures these predictions are compliant with KKT conditions, essential for guaranteeing the optimality of solutions in convex optimization under exam.

Network architecture is a MLP designed to take a batch of problem parameters $\Theta = \{\theta^{(i)}\}_{i=1}^N$ as input and predict $x^*$, $\lambda^*$, $\nu^*$. A ReLU function is placed at the end of the branch predicting $\lambda^*$ to ensure its feasability.

```{=tex}
\begin{align}
(\hat{x}, \hat{\lambda}, \hat{\nu}) &= \textrm{KINN}(\Theta)\\
\hat{\lambda} &\in \mathbb{R}⁰_+
\end{align}
```

Loss function is so defined:

$$
\mathcal{L} = \mathcal{L}_S + \sum_{i=1}^m\mathcal{L}_{I,i} + \mathcal{L}_E  + \sum_{i=1}^m\mathcal{L}_{C,i} 
$$
where:

```{=tex}
\begin{align}
    \mathcal{L}_S =& \|\nabla_{\hat{x}} f(\hat{x}, \theta) + \sum\nolimits_{i=1}^m \hat{\lambda}_i\nabla_{\hat{x}} g_i(\hat{x}, \theta) + A^T\hat{\nu}\|_2\\ 
    \mathcal{L}_{I,i}  =& \|\max(0, g_i(\hat{x}, \theta))\|_2\\
    \mathcal{L}_E =& \|A(\theta) \hat{x} - b(\theta)\|_2\\
    \mathcal{L}_{C,i}  =& \|\hat{\lambda}_i g_i(\hat{x}, \theta)\|_2\\
\end{align}
```
# Case study
Let us take such a problem as a test case for this approach:

We have a renewable energy generator in a power grid, whose active and reactive power injections are controllable. The set of injection points $(P,Q)$ is limited by physical constraints, so the set-points $(a_P, a_Q)$ must be projected onto that set.


## Problem description

The feasibile set $\mathcal{D}$ is defined by the physical parameters of the generator $\overline{P}_g, \underline{P}_g, P⁺_g, \overline{Q}_g, \underline{Q}_g, Q⁺_g, Q⁻_g$ , characterizing the minimum and maximum possible values and the relationships between active and reactive power, and the dynamic value $P^{\textrm{(max)}}_{g,t}$ which indicates the maximum power that can be generated at that time given the external conditions (e.g. wind speed, solar radiation, etc.):

\begin{equation}
\mathcal{D} = \{(P, Q) \in \mathbb{R}² | \underline{P}_g \leq P \leq P^{\textrm{(max)}}_{g,t}, Q_g \leq Q \leq \overline{Q}_g, Q \leq \tau^{(1)}_g P + \rho_g^{(1)}, Q \geq \tau^{(2)}_g P + \rho_g^{(2)}\}
\end{equation}

where:
```{=tex}
\begin{align}
    \tau^{(1)}_g &= \frac{Q_g⁺ - \overline{Q}_g}{\overline{P_g} - P_g⁺}\\
    \rho^{(1)}_g &= \overline{Q}_g - \tau^{(1)}_gP_g⁺\\
    \tau^{(2)}_g &= \frac{Q_g^- - \underline{Q}_g}{\overline{P_g} - P_g⁺}\\
    \rho^{(2)}_g &= \underline{Q}_g - \tau^{(2)}_gP_g⁺  \\
\end{align}
```
\begin{center}
\begin{tikzpicture}

% Assi


% Area grigia
\fill[gray!20] (0,-3) rectangle (5,3);
\fill[gray!20] (5,-2) rectangle (6,2);
\fill[gray!20] (5,2) -- (6,2) -- (5,3) -- cycle;
\fill[gray!20] (5,-2) -- (6,-2) -- (5,-3) -- cycle;
% Linee tratteggiate orizzontali per Q
\draw (0,3) -- (5,3);
\draw[dashed] (0,2) -- (6,2);
\draw[dashed] (0,-2) -- (6,-2);
\draw (0,-3) -- (5,-3);

% Linee tratteggiate verticali per P
\draw[dashed] (5,3) -- (5,-3);

% Etichette per Q
\node at (-0.3,3) {$Q_g$};
\node at (-0.5,2) {$Q_g^+$};
\node at (-0.3,-2) {$Q_g⁻$};
\node at (-0.5,-3) {$\underline{Q}_g$};

% Etichette per P
\node at (5,0.3) {$P_g^+$};
\node at (6,-0.3) {$\overline{P}_g$};

% Linee inclinate e relative etichette
\draw (5,3) -- (6,2);
\draw (5,-3) -- (6,-2);

\draw (6,1.5) -- (6,-2);
\node at (6.7,2) {$\tau_g^{(1)} P + \rho_g^{(1)}$};
\node at (6.7,-2) {$\tau_g^{(2)} P + \rho_g^{(2)}$};

\draw[->] (-0.5,0) -- (6.5,0) node[right] {$P$};
\draw[->] (0,-3.5) -- (0,3.5) node[above] {$Q$};
\end{tikzpicture}
\end{center}

The problem could be stated in standard form as:

$$
\begin{aligned}
\min_{x \in \mathcal{D} \subseteq\mathbb{R}^2} \quad &\frac{1}{2}\|a - x\|^{2}_{2} \\
\textrm{s.t.}\quad & G x - h \leq 0\\
\end{aligned}
$$

with $a = (a_P, a_Q)$, $x = (P, Q)$ and:

\begin{equation}
G = \begin{pmatrix}
-1 & 1 & 1 & 0 & 0 & -\tau^{(1)}_g & \tau^{(2)}_g\\
0 & 0 & 0 & -1 & 1 & 1 & 1
\end{pmatrix}^T
\end{equation}

\begin{equation}
h = \begin{pmatrix}
-\underline{P}_g & \overline{P}_g & P^{\textrm{(max)}}_{g,t} & -\underline{Q}_g & \overline{Q}_g & \rho^{(1)}_g & -\rho^{(2)}_g
\end{pmatrix}^T
\end{equation}

With associated KKT conditions:
\begin{align}
    G\hat{x} - h \leq 0&\\
    \lambda_i^* \geq 0& \quad i=1,\dots, 7\\
    G^T\hat\lambda = 0&\\
    (a-\hat{x}) + G^T\hat\lambda  = 0 &
\end{align}

## Experimental results
The problem described has a two-dimensional optimization variable, ten scalar parameters and seven constraints:

\begin{equation}
 (X, \Lambda) = \mathrm{KINN}(\Theta)
\end{equation}

with: 
\begin{align}
\Theta \in \mathbb{R}^{N \times 10}, \quad&\Theta_{i} = (a_P^{(i)}, a_Q^{(i)},\overline{P}_g^{(i)}, \underline{P}_g^{(i)}, P_g^{+^{(i)}}, \overline{Q}_g^{(i)}, \underline{Q}_g^{(i)}, Q_g^{+^{(i)}}, Q_g^{-^{(i)}}, P^{\textrm{(max)}^{(i)}}_{g,t})\\
X \in \mathbb{R}^{N \times 2}, \quad&X_{i} = (P^{(i)}, Q^{(i)})\\
\Lambda \in \mathbb{R}_+^{0^{N \times 7}}, \quad&\Lambda_{i} = \lambda^{(i)}
\end{align}

At each training step, a random batch of parameters $\Theta$ was sampled:

\begin{align}
a_P^{(i)} &\sim U(0~\mathrm{p.u}., 1~\mathrm{p.u.})\\
a_Q^{(i)} &\sim U(-1~\mathrm{p.u}., 1~\mathrm{p.u.})\\
\overline{P}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
P_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})\\
\overline{Q}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
Q_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{Q}_g^{(i)})\\
P^{\textrm{(max)}^{(i)}}_{g,t} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})
\end{align}

Models parameters were update to minimize the following loss function:

$$
\mathcal{L} = \frac{1}{N}\sum_{i=1}^N(\mathcal{L}_S^{(i)} + \mathcal{L}_{I}^{(i)}  + \mathcal{L}_{C}^{(i)})
$$
where:

```{=tex}
\begin{align}
    \mathcal{L}_S^{(i)} =& \|(a^{(i)}-\hat{x}^{(i)}) + G^{(i)^{T}}\hat\lambda^{(i)}\|_2\\ 
    \mathcal{L}_{I}^{(i)}  =& \|\max(0, G^{(i)}\hat{x} - h^{(i)})\|_2\\
    \mathcal{L}_{C}^{(i)}  =& \|G^{(i)^{T}}\hat\lambda^{(i)}\|_2\\
\end{align}
```

### Training
```{python}
#| label: fig-polar
#| echo: false
#| fig-cap: "A line plot on a polar axis"
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', palette='Set2')
sns.despine()

stat = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/stat.csv")
compl = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/compl.csv")
ineq = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/ineq.csv")
plt.yscale("log")
plt.plot(stat['Step'], stat['Value'].ewm(alpha=2 / 3).mean())
plt.plot(compl['Step'], compl['Value'].ewm(alpha=2 / 3).mean())
plt.plot(ineq['Step'], ineq['Value'].ewm(alpha=2 / 3).mean())

```


### Evaluation
```{python}
#| label: fig-polar2
#| echo: false
#| fig-cap: "A line plot on a polar axis"
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', palette='Set2')
sns.despine()

loss = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/loss.csv")
plt.yscale("log")
plt.plot(loss['Step'], loss['Value'].ewm(alpha=2 / 3).mean())

```

```{python}
#| label: fig-polar1
#| echo: false
#| fig-cap: "A line plot on a polar axis"
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', palette='Set2')
sns.despine()

r2 = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/r2.csv")[100:]
plt.plot(r2['Step'], r2['Value'].ewm(alpha=2 / 3).mean())

```
### Comparison
# Conclusions