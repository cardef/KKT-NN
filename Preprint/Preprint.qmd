---
title: KKT-Informed Neural Network
subtitle: A Parallel Solver for Parametric Convex Optimization Problem
author: 
    name: Carmine Delle Femine
    email: cdellefemine@vicomtech.org
    affiliation:
        name: Vicomtech Foundation
        department: Department of Data Intelligence for Energy and Industrial Processes
        address: Paseo Mikeletegi 57
        city: Donostia-San Sebastián
        region: Guipuzkoa
        state: Spain
        postal-code: 20009
        isni: 0000000460222780
        ror: https://ror.org/0023sah13
keywords:
    - Optimization
    - Parametric Optimization
    - Convex Optimization
    - Karush-Kuhn-Tucker (KKT) Conditions
    - Neural Networks
filters:
  - tikz
format:
    arxiv-pdf:
        keep-tex: true
        include-in-header:
            - text: |
                \usepackage{amsmath}
                \usepackage{tikz}
abstract: |
  It's presented a neural network-based approach for solving parametric convex optimization problems, where the network estimates the optimal points given a batch of input parameters. The network is trained by penalizing violations of the Karush-Kuhn-Tucker (KKT) conditions, ensuring that its predictions adhere to these optimality criteria. Additionally, since the bounds of the parameter space are known, training batches can be randomly generated without requiring external data. This method trades guaranteed optimality for significant improvements in speed, enabling parallel solving of a class of optimization problems.
---

# Introduction
Solving convex optimization problems is essential across numerous fields, including optimal control, logistics, and finance. In many scenarios, such as the development of surrogate models, there is a need to solve a large set of related optimization problems defined by varying parameters. Achieving fast solutions, even at the cost of strict optimality guarantees, is often a priority.

Neural networks, with their inherent ability to process data in parallel and adapt to diverse problem structures, offer a promising solution. In this work, we introduce the KKT-Informed Neural Network (KINN), a method designed to solve parametric convex optimization problems efficiently by integrating the KKT conditions into the network's learning process. By doing so, we enable the rapid, parallel solving of optimization problems while balancing the trade-off between speed and guaranteed optimality.

# State of the art
# Background

Consider a parametric convex optimization problem in the standard form:

$$
\begin{aligned}
\min_{x \in \mathcal{D} \subseteq\mathbb{R}^n} \quad &f(x, {\theta})\\
\textrm{s.t.} \quad & g_i(x, \theta) \leq 0 \quad i = 1, \dots, m \\
& A(\theta) x - b(\theta) = 0
\end{aligned}
$$

where $x \in \mathcal{D} \subseteq\mathbb{R}^n$ is the optimization variable; $\theta \in \mathcal{D}_\theta \subseteq \mathbb{R}^k$ are the parameters defining the problem; $f: \mathcal{D}_f \subseteq\mathbb{R}^n \times \mathbb{R}^k \to \mathbb{R}$ is the convex cost function; $g_i: \mathcal{D}_{g_i} \subseteq\mathbb{R}^n \times \mathbb{R}^k \to \mathbb{R}$ are the convex inequality constraints, $A: \mathcal{D}_\theta \to \mathbb{R}^{p \times n}$ and $b: \mathcal{D}_\theta \to \mathbb{R}^{p}$ defines the affine equality constraints and $\mathcal{D} = \bigcap_{i=1}^{m} \mathcal{D}_{g_i} \cap \mathcal{D}_{f}$ is the domain of the optimization problem.

Assume differentiable cost and constraints functions and that $g_i$ satisfies Slater's condition. Given a set of parameters $\theta$, $x^* \in \mathcal{D}$ is optimal if and only if there are $\lambda^*$ and $\nu^*$ that, with $x^*$, satisy the Karush-Kuhn-Tucker conditions (KKT):

```{=tex}
\begin{align}
    A(\theta) x^* - b(\theta) = 0&\\
    g_i(x^*, \theta) \leq 0& \quad i=1,\dots, m\\
    \lambda_i^* \geq 0& \quad i=1,\dots, m\\
    \lambda_i^* g_i(x^*, \theta) = 0& \quad i=1,\dots, m\\
    \nabla_{x^*} f(x^*, \theta) + \sum\nolimits_{i=1}^m \lambda^*_i\nabla_{x^*} g_i(x^*, \theta) + A(\theta)^T\nu^* = 0 &
\end{align}
```
# Proposed method

KKT-Informed Neural Network (KINN) builds upon the principles of Physics-Informed Neural Networks (PINNs), inducing compliance with Karush-Kuhn-Tucker (KKT) through a learning bias, directly coding their violation into the loss function that will be minimized in the training phase.

The network is designed as a multi-layer perceptron (MLP) and processes a batch of $B$ problem parameters $\Theta \in \mathbb{R}^{B \times k}$, $\Theta_i = \theta^{(i)}$. The network outputs $\hat{X}$, $\hat\Lambda$, $\hat{N}$. A ReLU function is placed at the end of the branch predicting $\hat\Lambda$ to ensure its feasability.

```{=tex}
\begin{align}
[\hat{X}, \hat{\Lambda}, \hat{N}] &= \textrm{KINN}(\Theta)\\
\hat{X} \in \mathbb{R}^{B\times n}&, \quad \hat X_i = x^{(i)}\\
\hat{\Lambda} \in \mathbb{R}^{0^{B\times m}}_+&, \quad \hat\Lambda_i = \lambda^{(i)}\\
\hat{N} \in \mathbb{R}^{B\times p}&, \quad \hat{N}_i = \nu^{(i)}
\end{align}
```

Vector-valued loss function consists of four terms that correspond to each KKT conditions:



\begin{equation}
\mathcal{L} = \frac{1}{B}[\sum_{i=1}^B\mathcal{L}_{S}^{(i)}, \sum_{i=1}^B\mathcal{L}_{I}^{(i)}, \sum_{i=1}^B\mathcal{L}_{E}^{(i)}, \sum_{i=1}^B\mathcal{L}_{C}^{(i)}] 
\end{equation}

where:

```{=tex}
\begin{align}
    \mathcal{L}_{S} =& \|\nabla_{\hat{x}^{(i)}} f(\hat{x}^{(i)}, \theta^{(i)}) + \sum\nolimits_{j=1}^m \hat{\lambda}^{(i)}_j\nabla_{\hat{x}^{(i)}} g_j(\hat{x}^{(i)}, \theta^{(i)}) + A(\theta^{(i)})^T\hat{\nu}^{(i)}\|_2\\ 
    \mathcal{L}_{I}^{(i)}  =& \|(\max(0, g_1(\hat{x}^{(i)}, \theta^{(i)})),\dots,\max(0, g_m(\hat{x}^{(i)}, \theta^{(i)})))\|_2\\
    \mathcal{L}_{E}^{(i)} =& \|A(\theta^{(i)}) \hat{x}^{(i)} - b(\theta^{(i)})\|_2\\
    \mathcal{L}_{C}^{(i)}  =& \|(\hat{\lambda}_1^{(i)} g_1(\hat{x}^{(i)}, \theta^{(i)}),\dots,\hat{\lambda}_m^{(i)} g_m(\hat{x}^{(i)}, \theta^{(i)}))\|_2\\
\end{align}
```

This vector-valued loss function is minimized through a Jacobian descent. Let $\mathcal{J} \in \mathbb{R}^{P\times4}$ the Jacobian matrix of $\mathcal{L}$, with $P$ the number of parameters of the KINN, $\mathcal{A}: \mathbb{R}^{P\times4} \to \mathbb{R}^{P}$ is called aggregator. The "direction" of the update of networks parameters will be $\mathcal{A}(\mathcal{J})$. The aggregator chosen is $\mathcal{A}_{\mathrm{UPGrad}}$. 

# Case study
We consider a renewable energy generator in a power grid as a test case for this approach. The generator's active and reactive power injections $(P,Q)$ are controllable, but they must adhere to physical constraints. As such, the desired setpoints $(a_P, a_Q)$ must be projected onto the feasible set defined by these constraints. This problem is taken from

## Problem description

The feasibile set $\mathcal{D}$ is defined by the physical parameters of the generator $\overline{P}_g \in \mathbb{R}_0⁺$, $P⁺_g \in ]0, \overline{P}_g]$, $\overline{Q}_g \in \mathbb{R}_0⁺$, $Q⁺_g \in ]0, \overline{Q}_g]$, characterizing the minimum and maximum possible values and the relationships between active and reactive power, and the dynamic value $P^{\textrm{(max)}}_{g,t}$ which indicates the maximum power that can be generated at that time given the external conditions (e.g. wind speed, solar radiation, etc.):

\begin{equation}
\mathcal{D} = \{(P, Q) \in \mathbb{R}² | 0 \leq P \leq P^{\textrm{(max)}}_{g,t}, -\overline{Q}_g \leq Q \leq \overline{Q}_g, Q \leq \tau^{(1)}_g P + \rho_g^{(1)}, Q \geq \tau^{(2)}_g P + \rho_g^{(2)}\}
\end{equation}

where:
```{=tex}
\begin{align}
    \tau^{(1)}_g &= \frac{Q_g⁺ - \overline{Q}_g}{\overline{P_g} - P_g⁺}\\
    \rho^{(1)}_g &= \overline{Q}_g - \tau^{(1)}_gP_g⁺\\
    \tau^{(2)}_g &= \frac{\overline{Q}_g -Q_g^+ }{\overline{P_g} - P_g⁺}\\
    \rho^{(2)}_g &= -\overline{Q}_g - \tau^{(2)}_gP_g⁺  \\
\end{align}
```

```{=tex}
\begin{figure}

\begin{center}
\begin{tikzpicture}

% Assi


% Area grigia
\fill[gray!20] (0,-3) rectangle (4,3);
\fill[gray!20] (4,-2) rectangle (5,2);
\fill[gray!20] (4,2) -- (5,2) -- (5, 2.5) -- (4,3)-- cycle;
\fill[gray!20] (4,-2) -- (5,-2) -- (5, -2.5) -- (4,-3) -- cycle;
% Linee tratteggiate orizzontali per Q
\draw (0,3) -- (4,3);
\draw[dashed] (0,2) -- (6,2);
\draw[dashed] (0,-2) -- (6,-2);
\draw (0,-3) -- (4,-3);

% Linee tratteggiate verticali per P
\draw[dashed] (4,3) -- (4,-3);
\draw (5,2.5) -- (5,-2.5);
% Etichette per Q
\node at (-0.5,3) {$\overline{Q}_g$};
\node at (-0.5,2) {$Q_g^+$};
\node at (-0.5,-2) {$-Q_g⁺$};
\node at (-0.5,-3) {$-\overline{Q}_g$};

% Etichette per P
\node at (3.7,0.3) {$P_g^+$};
\node at (5.5,0.3) {$P^{\textrm{(max)}}_{g,t}$};
\node at (6.3,-0.3) {$\overline{P}_g$};

% Linee inclinate e relative etichette
\draw (4,3) -- (5,2.5);
\draw (4,-3) -- (5,-2.5);
\draw[dashed] (4,3) -- (6,2);
\draw[dashed] (4,-3) -- (6,-2);

\draw[dashed] (6,2) -- (6,-2);
\node at (5.5,3) {$\tau_g^{(1)} P + \rho_g^{(1)}$};
\node at (5.5,-3) {$\tau_g^{(2)} P + \rho_g^{(2)}$};

\draw[->] (-0.5,0) -- (6.5,0) node[right] {$P$};
\draw[->] (0,-3.5) -- (0,3.5) node[above] {$Q$};
\end{tikzpicture}
\end{center}
\caption{A picture of a gull}
\end{figure}
```
The problem could be stated in standard form as:

$$
\begin{aligned}
\min_{x \in \mathcal{D} \subseteq\mathbb{R}^2} \quad &\frac{1}{2}\|a - x\|^{2}_{2} \\
\textrm{s.t.}\quad & G x - h \leq 0\\
\end{aligned}
$$

with $a = (a_P, a_Q)$, $x = (P, Q)$ and:

\begin{equation}
G = \begin{pmatrix}
-1 & 1 & 1 & 0 & 0 & -\tau^{(1)}_g & \tau^{(2)}_g\\
0 & 0 & 0 & -1 & 1 & 1 & 1
\end{pmatrix}^T
\end{equation}

\begin{equation}
h = \begin{pmatrix}
0 & \overline{P}_g & P^{\textrm{(max)}}_{g,t} & \overline{Q}_g & \overline{Q}_g & \rho^{(1)}_g & -\rho^{(2)}_g
\end{pmatrix}^T
\end{equation}

With associated KKT conditions:
\begin{align}
    G\hat{x} - h \leq 0&\\
    \lambda_i^* \geq 0& \quad i=1,\dots, 7\\
    G^T\hat\lambda = 0&\\
    (a-\hat{x}) + G^T\hat\lambda  = 0 &
\end{align}

## Experimental results
The problem described has a two-dimensional optimization variable, seven scalar parameters and seven constraints:

\begin{equation}
 (X, \Lambda) = \mathrm{KINN}(\Theta)
\end{equation}

with: 
\begin{align}
\Theta \in \mathbb{R}^{B \times 7}, \quad&\Theta_{i} = (a_P^{(i)}, a_Q^{(i)},\overline{P}_g^{(i)},  P_g^{+^{(i)}}, \overline{Q}_g^{(i)},  Q_g^{+^{(i)}}, P^{\textrm{(max)}^{(i)}}_{g,t})\\
X \in \mathbb{R}^{B \times 2}, \quad&X_{i} = (P^{(i)}, Q^{(i)})\\
\Lambda \in \mathbb{R}_+^{0^{B \times 7}}, \quad&\Lambda_{i} = \lambda^{(i)}
\end{align}

The network is composed by two hidden layers of 512 neurons each, with a LeakyReLU (negative slope of $0.01$) as activation function.  

At each training step, a random batch of parameters $\Theta$ was sampled:

\begin{align}
a_P^{(i)} &\sim U(0~\mathrm{p.u}., 1~\mathrm{p.u.})\\
a_Q^{(i)} &\sim U(-1~\mathrm{p.u}., 1~\mathrm{p.u.})\\
\overline{P}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
P_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})\\
\overline{Q}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
Q_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{Q}_g^{(i)})\\
P^{\textrm{(max)}^{(i)}}_{g,t} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})
\end{align}

Models parameters were update to minimize the following vector-valued loss function:

\begin{equation}
\mathcal{L} = \frac{1}{B}[\sum_{i=1}^B\mathcal{L}_{S}^{(i)}, \sum_{i=1}^B\mathcal{L}_{I}^{(i)}, \sum_{i=1}^B\mathcal{L}_{C}^{(i)}] 
\end{equation}

where:

```{=tex}
\begin{align}
    \mathcal{L}_S^{(i)} =& \|(a^{(i)}-\hat{x}^{(i)}) + G^{(i)^{T}}\hat\lambda^{(i)}\|_2\\ 
    \mathcal{L}_{I}^{(i)}  =& \|\max(0, G^{(i)}\hat{x} - h^{(i)})\|_2\\
    \mathcal{L}_{C}^{(i)}  =& \|G^{(i)^{T}}\hat\lambda^{(i)}\|_2\\
\end{align}
```

### Training

Training was performed with the Adam optimization algorithm with an intial learning rate of $10⁻3$ and an exponential scheduler for the latter with a $\gamma$ of $0.997$. An early stopping condition occurred when no progress occurred on any of the constituent terms of the loss for $5000$ steps.

Finally, the training lasted reaching final values shown in the table
```{python}
#| label: fig-loss
#| echo: false
#| fig-cap: "Loss terms during training"
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', palette='Set2')
sns.despine()

stat = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/stat.csv")
compl = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/compl.csv")
ineq = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/ineq.csv")
plt.yscale("log")
plt.plot(stat['Step'], stat['Value'].ewm(alpha=2 / 3).mean())
plt.plot(compl['Step'], compl['Value'].ewm(alpha=2 / 3).mean())
plt.plot(ineq['Step'], ineq['Value'].ewm(alpha=2 / 3).mean())

```


### Evaluation

To evaluate the approach presented here, the “cvxpylayers” library, which implements a batched solver, was used as a baseline
```{python}
#| label: fig-polar2
#| echo: false
#| fig-cap: "A line plot on a polar axis"
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', palette='Set2')
sns.despine()

loss = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/loss.csv")
plt.yscale("log")
plt.plot(loss['Step'], loss['Value'].ewm(alpha=2 / 3).mean())

```

```{python}
#| label: fig-polar1
#| echo: false
#| fig-cap: "A line plot on a polar axis"
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', palette='Set2')
sns.despine()

r2 = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/r2.csv")[100:]
plt.plot(r2['Step'], r2['Value'].ewm(alpha=2 / 3).mean())

```

### Comparison
```{python}
#| label: fig-polar4
#| echo: false
#| fig-cap: "A line plot on a polar axis"
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='ticks', palette='Set2')
sns.despine()

times = pd.read_csv("/home/VICOMTECH/cdellefemine/Documents/code/KKT-NN/Preprint/plots/times.csv")
plt.yscale("log")
plt.plot(times['size'], times['KINN'].ewm(alpha=2 / 3).mean())
plt.plot(times['size'], times['CVXPY'].ewm(alpha=2 / 3).mean())

```
# Conclusions