---
title: KKT-Informed Neural Network
subtitle: A Parallel Solver for Parametric Convex Optimization Problems
author: 
    name: 
      given: Carmine
      family: Delle Femine
    orcid: 0009-0005-7964-8911
    email: cdellefemine@vicomtech.org
    affiliation:
        name: Vicomtech Foundation
        department: Department of Data Intelligence for Energy and Industrial Processes
        address: Paseo Mikeletegi 57
        city: Donostia-San Sebastián
        region: Guipuzkoa
        state: Spain
        postal-code: 20009
        isni: 0000000460222780
        ror: https://ror.org/0023sah13
keywords:
    - Optimization
    - Parametric Optimization
    - Convex Optimization
    - Karush-Kuhn-Tucker (KKT) Conditions
    - Neural Networks

date: 11 September 2024
date-format: long
format:
    arxiv-pdf:
        pdf-engine: pdflatex
        pdf-engine-opts:
          - --shell-escape
        fig-format: svg
        fig-align: center
        fig.width: 6
        fig.asp: 0.618
        knitr:
            opts_chunk:
                fig-asp: 0.618
                message: False
                fig-width: 6
                out-width: "70%"
                fig-align: center
                dev: svg
                dpi: 600
        cite-method: natbib
        natbiboptions: numbers
        keep-tex: true
        include-in-header:
            - text: |
                \usepackage{amsmath}
                \usepackage{tikz}
                \usepackage{xcolor}
                \usepackage{svg}
abstract: |
    A neural network-based approach for solving parametric convex optimization problems is presented, where the network estimates the optimal points given a batch of input parameters. The network is trained by penalizing violations of the Karush-Kuhn-Tucker (KKT) conditions, ensuring that its predictions adhere to these optimality criteria. Additionally, since the bounds of the parameter space are known, training batches can be randomly generated without requiring external data. This method trades guaranteed optimality for significant improvements in speed, enabling parallel solving of a class of optimization problems.
bibliography: Preprint.bib
execute:
  cache: True 
  debug: true

---
# Introduction
Solving convex optimization problems is essential across numerous fields, including optimal control, logistics, and finance. In many scenarios, such as the development of surrogate models, there is a need to solve a large set of related optimization problems defined by varying parameters. Achieving fast solutions, even at the cost of strict optimality guarantees, is often a priority.

Neural networks, with their inherent ability to process data in parallel and adapt to diverse problem structures, offer a promising solution. This work introduces the KKT-Informed Neural Network (KINN), a method designed to solve parametric convex optimization problems efficiently by integrating the KKT conditions into the network's learning process. This approach enables rapid, parallel problem-solving while balancing the trade-off between speed and guaranteed optimality.

# Background

Consider a parametric convex optimization problem in the standard form:

$$
\begin{aligned}
\min_{x \in \mathcal{D} \subseteq\mathbb{R}^n} \quad &f(x, {\theta})\\
\textrm{s.t.} \quad & g_i(x, \theta) \leq 0 \quad i = 1, \dots, m \\
& A(\theta) x - b(\theta) = 0
\end{aligned}
$$

where $x \in \mathcal{D} \subseteq\mathbb{R}^n$ is the optimization variable; $\theta \in \mathcal{D}_\theta \subseteq \mathbb{R}^k$ are the parameters defining the problem; $f: \mathcal{D}_f \subseteq\mathbb{R}^n \times \mathbb{R}^k \to \mathbb{R}$ is the convex cost function; $g_i: \mathcal{D}_{g_i} \subseteq\mathbb{R}^n \times \mathbb{R}^k \to \mathbb{R}$ are the convex inequality constraints, $A: \mathcal{D}_\theta \to \mathbb{R}^{p \times n}$ and $b: \mathcal{D}_\theta \to \mathbb{R}^{p}$ defines the affine equality constraints and $\mathcal{D} = \bigcap_{i=1}^{m} \mathcal{D}_{g_i} \cap \mathcal{D}_{f}$ is the domain of the optimization problem.

Assume differentiable cost and constraints functions and that $g_i$ satisfies Slater's condition. Given a set of parameters $\theta$, $x^* \in \mathcal{D}$ is optimal if and only if there are $\lambda^*$ and $\nu^*$ that, with $x^*$, satisy the Karush-Kuhn-Tucker conditions (KKT) [@boydConvexOptimization2004]:

```{=tex}
\begin{align}
    A(\theta) x^* - b(\theta) = 0&\\
    g_i(x^*, \theta) \leq 0& \quad i=1,\dots, m\\
    \lambda_i^* \geq 0& \quad i=1,\dots, m\\
    \lambda_i^* g_i(x^*, \theta) = 0& \quad i=1,\dots, m\\
    \nabla_{x^*} f(x^*, \theta) + \sum\nolimits_{i=1}^m \lambda^*_i\nabla_{x^*} g_i(x^*, \theta) + A(\theta)^T\nu^* = 0 &
\end{align}
```
# State of the art
# Proposed method



![KINN architecture](kinn.svg){#fig-architecture width=6in fig-align="center"}

KKT-Informed Neural Network (KINN) builds upon the principles of Physics-Informed Neural Networks (PINNs) [@raissiPhysicsinformedNeuralNetworks2019a], inducing compliance with Karush-Kuhn-Tucker (KKT) through a learning bias, directly coding their violation into the loss function that will be minimized in the training phase.


The objective is to process a batch of $B$ problem parameters $\Theta \in \mathbb{R}^{B \times k}$, $\Theta_i = \theta^{(i)}$ and produce the outputs $\hat{X}$, $\hat\Lambda$, $\hat{N}$.
Assuming that the range of each parameter is known a priori, $\Theta$  is linearly scaled to obtain a normalized parameter batch $\Theta^\prime \in [-1 , 1]^{B \times k}$ , which serves as the input to the neural network to facilitate numerical stability and enhance convergence during training. The network architecture consists of a Multi-Layer Perceptron (MLP) with skip connections, which produces a shared embedding of the scaled parameter batch $\Theta^\prime$. This shared embedding is then used as input to three separate MLPs, each tasked with predicting one of the outputs: the optimal primal variables $\hat{X}$ and the dual variables $\hat{\Lambda}$, $\hat{N}$. The shared embedding ensures that the network captures the common structure of the parameter space, while the individual MLPs specialize in their respective prediction tasks, thereby improving both training efficiency and model accuracy. To ensure feasibility, a softplus activation function is applied to $\hat{\Lambda}$.

```{=tex}
\begin{align}
[\hat{X}, \hat{\Lambda}, \hat{N}] &= \textrm{KINN}(\Theta)\\
\hat{X} \in \mathbb{R}^{B\times n}&, \quad \hat X_i = \hat{x}^{(i)}\\
\hat{\Lambda} \in \mathbb{R}^{+^{B\times m}}_0&, \quad \hat\Lambda_i = \hat\lambda^{(i)}\\
\hat{N} \in \mathbb{R}^{B\times p}&, \quad \hat{N}_i = \hat\nu^{(i)}
\end{align}
```

Vector-valued loss function consists of four terms that correspond to each KKT conditions:



\begin{equation}
\mathcal{L} = \frac{1}{B}\biggr[\sum_{i=1}^B\mathcal{L}_{S}^{(i)}, \sum_{i=1}^B\mathcal{L}_{I}^{(i)}, \sum_{i=1}^B\mathcal{L}_{E}^{(i)}, \sum_{i=1}^B\mathcal{L}_{C}^{(i)}\biggr] 
\end{equation}

where:

```{=tex}
\begin{align}
    \mathcal{L}_{S}^{(i)} =& \|\nabla_{\hat{x}^{(i)}} f(\hat{x}^{(i)}, \theta^{(i)}) + \sum\nolimits_{j=1}^m \hat{\lambda}^{(i)}_j\nabla_{\hat{x}^{(i)}} g_j(\hat{x}^{(i)}, \theta^{(i)}) + A(\theta^{(i)})^T\hat{\nu}^{(i)}\|_2\\ 
    \mathcal{L}_{I}^{(i)}  =& \|(\max(0, g_1(\hat{x}^{(i)}, \theta^{(i)})),\dots,\max(0, g_m(\hat{x}^{(i)}, \theta^{(i)})))\|_2\\
    \mathcal{L}_{E}^{(i)} =& \|A(\theta^{(i)}) \hat{x}^{(i)} - b(\theta^{(i)})\|_2\\
    \mathcal{L}_{C}^{(i)}  =& \|(\hat{\lambda}_1^{(i)} g_1(\hat{x}^{(i)}, \theta^{(i)}),\dots,\hat{\lambda}_m^{(i)} g_m(\hat{x}^{(i)}, \theta^{(i)}))\|_2\\
\end{align}
```

This vector-valued loss function is minimized through a Jacobian descent [@quintonJacobianDescentMultiObjective2024a]. Let $\mathcal{J} \in \mathbb{R}^{P\times4}$ the Jacobian matrix of $\mathcal{L}$, with $P$ the number of parameters of the KINN, $\mathcal{A}: \mathbb{R}^{P\times4} \to \mathbb{R}^{P}$ is called aggregator. The "direction" of the update of networks parameters will be $\mathcal{A}(\mathcal{J})$. The aggregator chosen is $\mathcal{A}_{\mathrm{UPGrad}}$, described in [@quintonJacobianDescentMultiObjective2024a]. 
{{< pagebreak >}}
# Case studies
## Portfolio optimization

Portfolio optimization is a classic problem in quantitative finance, aimed at the allocation of capital across a set of financial assets to achieve a predefined objective while satisfying specific constraints. Portfolio optimization is typically based on Markowitz model: given $N$ securities and assuming the knowledge of excpected return of each security $\mu \in \mathbb{R}^N$ and of the covariance matrix of returns, used to model risks associated with the securities, $\Sigma \in \mathbb{R}^{N\times N}$, we want to find the vector $w \in [0, 1]^N$, where $w_i$ is the fraction of funds invested into security $i$, such that:

$$
\begin{aligned}
\max_{w \in [0, 1]^N } \quad & w^T\mu - \frac{\delta}{2}w^T\Sigma w \\
\textrm{s.t.}\quad & \sum_{i=1}^N w_i = 1\\
\end{aligned}
$$

with $\delta \in \mathbb{R}^+$ a risk aversion parameter.

### Problem description

The case study is based on the price history of 20 assets. The expected return value of each asset is estimated with the geometric mean of its annual increments, while a Ledoit-Wolf shrinkage estimate is used as the covariance matrix. To make the problem more complex, constraints were added on the funds to be allocated defined by six parameters:

- $\overline{T}$: maximum fraction of funds allocated in technological assets, corresponding to the indices $1, 2, 3, 4, 5, 7$,
- $\overline{R}$: maximum fraction of funds allocated in retail assets, corresponding to the indices $8, 13, 16, 20$,
- $\overline{F}$: maximum fraction of funds allocated in financial institutions group, corresponding to the indices $9, 17, 19$,
- $\overline{E}$: maximum fraction of funds allocated in energy assets, corresponding to the indices $14, 15$,
- U: exact fraction of funds allocated in utility assets, corresponding to the index $6$,
- A: exact fraction of funds allocated in airline assets, corresponding to the index $12$.

Hence, the problem could be stated in standard form as:

$$
\begin{aligned}
\min_{w \in [0, 1]^N } \quad & - w^T\mu + \frac{\delta}{2}w^T\Sigma w \\
\textrm{s.t.}\quad & \sum_{i=1}^N w_i - 1 = 0\\
& w_6 - U = 0\\
& w_{12} - A = 0\\
&w_1 + w_2 + w_3 + w_4 + w_5 + w_7 - \overline{T} \leq 0\\
&w_8 + w_2 + w_{13} + w_{16} + w_{20} - \overline{R} \leq 0\\
&w_9 + w_{17} + w_{19} - \overline{F} \leq 0\\
&w_{14} + w_{15} - \overline{E} \leq 0
\end{aligned}
$${#eq-portfolio_standard_form}


### Experimental results

The problem described has a 20-dimensional decision variable, 7 scalar parameters, 3 equality constraints and 4 inequality constraints:

\begin{equation}
 [\hat{X}, \hat\Lambda, \hat N] = \mathrm{KINN}(\Theta^\prime)
\end{equation}

with:

\begin{align}
\Theta^\prime \in \mathbb{R}^{B \times 7}, \quad&\Theta^\prime_{i} = (\delta^{(i)}, \overline{T}^{(i)},\overline{R}^{(i)},  \overline{E}^{(i)}, \overline{F}^{(i)},  U^{(i)}, A^{(i)})\\
\hat{X} \in \mathbb{R}^{B \times 20}, \quad&\hat{X}_{i} = \hat{x}^{(i)} = (w_1, \dots, w_{20})\\
\hat\Lambda \in \mathbb{R}_0^{+^{B \times 4}}, \quad&\hat\Lambda_{i} = \hat\lambda^{(i)}\\
\hat N \in \mathbb{R}^{{B \times 3}}, \quad&\hat N_{i} = \hat \nu^{(i)}
\end{align}

At each training step, a random batch of parameters $\Theta$ was sampled:

\begin{align}
a_P^{(i)} &\sim U(0~\mathrm{p.u}., 1~\mathrm{p.u.})\\
a_Q^{(i)} &\sim U(-1~\mathrm{p.u}., 1~\mathrm{p.u.})\\
\overline{P}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
P_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})\\
\overline{Q}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
Q_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{Q}_g^{(i)})\\
P^{\textrm{(max)}^{(i)}}_{g,t} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})
\end{align}

The neural network was defined to have four residual layers in each of the branches, the embedding layer and the three output layers. Each layer consists of 32 neurons.

#### Training
#### Evaluation

## Model predictive control
### Problem description
### Experimental results
#### Training
#### Evaluation


## Projection onto a convex polytope
A renewable energy generator in a power grid is used as a test case for this approach. The generator's active and reactive power injections $(P,Q)$ are controllable, but they must adhere to physical constraints. As such, the desired setpoints $(a_P, a_Q)$ must be projected onto the feasible set defined by these constraints. This problem is taken from [@henryGymANMReinforcementLearning2021].

### Problem description
The feasible set $\mathcal{D}$ (shown in Figure $\ref{fig:set}$) is defined by the physical parameters of the generator $\overline{P}_g \in \mathbb{R}_0^+$, $P^+_g \in ]0, \overline{P}_g]$, $\overline{Q}_g \in \mathbb{R}_0^+$, $Q^+_g \in ]0, \overline{Q}_g]$, characterizing the minimum and maximum possible values, the relationships between active and reactive power, and the dynamic value $P^{\textrm{(max)}}_{g,t}$ which indicates the maximum power that can be generated at that time given the external conditions (e.g. wind speed, solar radiation, etc.):
\begin{equation}
\mathcal{D} = \{(P, Q) \in \mathbb{R}^2 | 0 \leq P \leq P^{\textrm{(max)}}_{g,t}, -\overline{Q}_g \leq Q \leq \overline{Q}_g, Q \leq \tau^{(1)}_g P + \rho_g^{(1)}, Q \geq \tau^{(2)}_g P + \rho_g^{(2)}\}
\end{equation}

where:
```{=tex}
\begin{align}
    \tau^{(1)}_g &= \frac{Q_g^+ - \overline{Q}_g}{\overline{P_g} - P_g^+}\\
    \rho^{(1)}_g &= \overline{Q}_g - \tau^{(1)}_gP_g^+\\
    \tau^{(2)}_g &= \frac{\overline{Q}_g -Q_g^+ }{\overline{P_g} - P_g^+}\\
    \rho^{(2)}_g &= -\overline{Q}_g - \tau^{(2)}_gP_g^+  \\
\end{align}
```

```{=tex}
\begin{figure}
\begin{center}
\begin{tikzpicture}

% Assi


% Area grigia
\fill[gray!20] (0,-3) rectangle (4,3);
\fill[gray!20] (4,-2) rectangle (5,2);
\fill[gray!20] (4,2) -- (5,2) -- (5, 2.5) -- (4,3)-- cycle;
\fill[gray!20] (4,-2) -- (5,-2) -- (5, -2.5) -- (4,-3) -- cycle;
% Linee tratteggiate orizzontali per Q
\draw (0,3) -- (4,3);
\draw[dashed] (0,2) -- (6,2);
\draw[dashed] (0,-2) -- (6,-2);
\draw (0,-3) -- (4,-3);

% Linee tratteggiate verticali per P
\draw[dashed] (4,3) -- (4,-3);
\draw (5,2.5) -- (5,-2.5);
% Etichette per Q
\node at (-0.5,3) {$\overline{Q}_g$};
\node at (-0.5,2) {$Q_g^+$};
\node at (-0.5,-2) {$-Q_g^+$};
\node at (-0.5,-3) {$-\overline{Q}_g$};

% Etichette per P
\node at (3.7,0.3) {$P_g^+$};
\node at (5.5,0.3) {$P^{\textrm{(max)}}_{g,t}$};
\node at (6.3,-0.3) {$\overline{P}_g$};

% Linee inclinate e relative etichette
\draw (4,3) -- (5,2.5);
\draw (4,-3) -- (5,-2.5);
\draw[dashed] (4,3) -- (6,2);
\draw[dashed] (4,-3) -- (6,-2);

\draw[dashed] (6,2) -- (6,-2);
\node at (5.5,3) {$\tau_g^{(1)} P + \rho_g^{(1)}$};
\node at (5.5,-3) {$\tau_g^{(2)} P + \rho_g^{(2)}$};

\draw[->] (-0.5,0) -- (6.5,0) node[right] {$P$};
\draw[->] (0,-3.5) -- (0,3.5) node[above] {$Q$};
\end{tikzpicture}
\end{center}
\caption{Feasibile set $\mathcal{D}$} \label{fig:set}
\end{figure}
```


The problem could be stated in standard form as:

$$
\begin{aligned}
\min_{x \in \mathcal{D} \subseteq\mathbb{R}^2} \quad &\frac{1}{2}\|a - x\|^{2}_{2} \\
\textrm{s.t.}\quad & G x - h \leq 0\\
\end{aligned}
$$

with $a = (a_P, a_Q)$, $x = (P, Q)$ and:

\begin{equation}
G = \begin{pmatrix}
-1 & 1 & 1 & 0 & 0 & -\tau^{(1)}_g & \tau^{(2)}_g\\
0 & 0 & 0 & -1 & 1 & 1 & 1
\end{pmatrix}^T
\end{equation}

\begin{equation}
h = \begin{pmatrix}
0 & \overline{P}_g & P^{\textrm{(max)}}_{g,t} & \overline{Q}_g & \overline{Q}_g & \rho^{(1)}_g & -\rho^{(2)}_g
\end{pmatrix}^T
\end{equation}

With associated KKT conditions:
\begin{align}
    G{x}^* - h \leq 0&\\
    \lambda_i^* \geq 0& \quad i=1,\dots, 7\\
    G^T\lambda^* = 0&\\
    (a-{x}^*) + G^T\lambda^*  = 0 &
\end{align}

### Experimental results
The problem described has a two-dimensional decision variable, seven scalar parameters and seven inequality constraints:

\begin{equation}
 [\hat{X}, \hat\Lambda] = \mathrm{KINN}(\Theta^\prime)
\end{equation}

with: 
\begin{align}
\Theta^\prime \in \mathbb{R}^{B \times 7}, \quad&\Theta^\prime_{i} = (a_P^{(i)}, a_Q^{(i)},\overline{P}_g^{(i)},  P_g^{+^{(i)}}, \overline{Q}_g^{(i)},  Q_g^{+^{(i)}}, P^{\textrm{(max)}^{(i)}}_{g,t})\\
\hat{X} \in \mathbb{R}^{B \times 2}, \quad&\hat{X}_{i} = \hat{x}^{(i)} = (\hat{P}^{(i)}, \hat{Q}^{(i)})\\
\hat\Lambda \in \mathbb{R}_0^{+^{B \times 7}}, \quad&\hat\Lambda_{i} = \hat\lambda^{(i)}
\end{align}

| Parameter            | Range  |
|:---------------:|:------:|
| $a_P$ | $[0 ,1]$ |
| $a_Q$ | $[-1 ,1]$ |
| $\overline{P}_g$ | $[0.2 ,0.8]$ |
| $P_g^{+}$ | $[0 ,\overline{P}_g]$ |
| $\overline{Q}_g$ | $[0.2 ,0.8]$ |
| $Q_g^{+}$ | $[0 ,\overline{Q}_g]$ |
| $P^{\textrm{(max)}}_{g}$ | $[0 ,\overline{P}_g]$ |
: Parameters range {#tbl-proj-parameters}

The network is composed by three hidden layers of $512$ neurons each, with a LeakyReLU (negative slope of $0.01$) as activation function and a skip connection around each hidden layer.  

At each training step, a random batch of parameters $\Theta$ was sampled:

\begin{align}
a_P^{(i)} &\sim U(0~\mathrm{p.u}., 1~\mathrm{p.u.})\\
a_Q^{(i)} &\sim U(-1~\mathrm{p.u}., 1~\mathrm{p.u.})\\
\overline{P}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
P_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})\\
\overline{Q}_g^{(i)} &\sim U(0.2~\mathrm{p.u}., 0.8~\mathrm{p.u.})\\
Q_g^{+^{(i)}} &\sim U(0~\mathrm{p.u}., \overline{Q}_g^{(i)})\\
P^{\textrm{(max)}^{(i)}}_{g,t} &\sim U(0~\mathrm{p.u}., \overline{P}_g^{(i)})
\end{align}

Models parameters were update to minimize the following vector-valued loss function:

\begin{equation}
\mathcal{L} = \frac{1}{B}\biggr[\sum_{i=1}^B\mathcal{L}_{S}^{(i)}, \sum_{i=1}^B\mathcal{L}_{I}^{(i)}, \sum_{i=1}^B\mathcal{L}_{C}^{(i)}\biggr] 
\end{equation}

where:

```{=tex}
\begin{align}
    \mathcal{L}_S^{(i)} =& \|(a^{(i)}-\hat{x}^{(i)}) + G^{(i)^{T}}\hat\lambda^{(i)}\|_2\\ 
    \mathcal{L}_{I}^{(i)}  =& \|\max(0, G^{(i)}\hat{x} - h^{(i)})\|_2\\
    \mathcal{L}_{C}^{(i)}  =& \|G^{(i)^{T}}\hat\lambda^{(i)}\|_2\\
\end{align}
```

#### Training

Training was performed with the Adam optimization algorithm with an intial learning rate of $10^{-3}$ and an exponential scheduler for the latter with a $\gamma$ of $0.99986$. An early stopping condition occurred when no progress occurred on any of the constituent terms of the loss for $5000$ steps.

Finally, the training lasted $3583$ steps reaching final values shown in @tbl-training, while the evolution along the various steps is in @fig-training.

| Loss            | Value  |
|:---------------:|:------:|
| $\mathcal{L}_S$ | 0.3519 |
| $\mathcal{L}_I$ | 0.0020 |
| $\mathcal{L}_C$ | 0.0000 |
: Final values {#tbl-training}


```{r}
#| output: false
#| echo: false
library(tidyverse)
library(ggpubr)
library(ggthemes)
library(svglite)
library(GGally)
library(akima)
library(hrbrthemes)
library(extrafont)
library(scales)
library(latex2exp)


```

```{r}
#| label: fig-training
#| echo: false
#| fig-cap: "Loss terms during training"
log_df <- read_csv("../Projection/projection_losses.csv", show_col_types=FALSE)
log_df <- log_df%>% pivot_longer(!Step, names_to="Loss", values_to="Value")

extrafont::loadfonts(quiet = TRUE)
ggplot(log_df, aes(x=Step, y=Value, color=Loss)) + geom_line()   + scale_y_continuous(transform = "pseudo_log")+scale_colour_tableau(
  palette = "Tableau 10", direction=-1
) + theme_light()
```


```{r}
#| label: fig-params
#| echo: false
#| fig-cap: "Loss terms during training"
#| fig-asp: 1.0
#| fig-width: 10
#| out-width: "100%"
params_df <- read_csv("../Projection/param_analysis.csv", show_col_types=FALSE)
ggally_mysmooth <- function(data, mapping, ...){
    vars = unique(unlist(lapply(mapping, all.vars)))
    grid <- with(data, interp(x = data[[vars[1]]], y = data[[vars[2]]], z = data[[vars[3]]], linear = TRUE, extrap = FALSE,
                            xo = seq(min(data[[vars[1]]]), max(data[[vars[1]]]), length = 100), 
                            yo = seq(min(data[[vars[2]]]), max(data[[vars[2]]]), length = 100)))

    df <- as.data.frame(interp2xyz(grid))
    ggplot(data = df, mapping=aes(x,y, fill=z)) +geom_raster(interpolate=TRUE, show.legend=TRUE)   +  scale_fill_gradient2_tableau(palette = "Orange-Blue Diverging", transform=c("log10", "reverse"), guide=guide_colourbar(reverse = TRUE), name = "Residuals sum") + theme_light() 
}
ggpairs(params_df, mapping=aes(fill=sum),columns = 1:7, upper = list(continuous = "blank"), diag = list(continuous = "blankDiag"), lower = list(continuous = ggally_mysmooth), legend = 8, columnLabels = c("a[P]", "a[Q]", "P[g * list(, ) * t]^{
    (max)
}", "bar(
    P
)[g]", "bar(Q)[g]", "P[g]^{
    phantom() + phantom()
}","Q[g]^{
    phantom() + phantom()
}"), labeller = "label_parsed") + theme(legend.position="bottom") + theme_classic() + theme(axis.line = element_blank(), panel.border = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
```

#### Evaluation

To evaluate the approach presented here, the “cvxpylayers” library [@agrawalDifferentiableConvexOptimization2019a], which implements a batched solver via multi-threading, was used as a baseline. The validation set consists of 1000 samples, generated by taking the physical parameters of the two generators present in the use case presented in [@henryGymANMReinforcementLearning2021] and, for each of them, simulating 500 random inputs $(a_P, a_Q)$ and external condition $P^{\textrm{(max)}}_{g,t}$. Specifically these parameters are:

\begin{align}
    [\overline{P}_g^{(i)},  P_g^{+^{(i)}}, \overline{Q}_g^{(i)},  Q_g^{+^{(i)}}]_1 &= [0.3, 0.2, 0.3, 0.15]\\
    [\overline{P}_g^{(i)},  P_g^{+^{(i)}}, \overline{Q}_g^{(i)},  Q_g^{+^{(i)}}]_2 &= [0.5, 0.35, 0.5, 0.2]
\end{align}

The metrics for validation were the mean absolute error (MAE) and $R^2$. Last values are shown in  [@tbl-eval], , while the evolution along the various steps is in [@fig-val].

By increasing the number of points, an inference time comparison was performed on an Apple M2 Pro processor with backend for PyTorch's MPS. The difference is remarkable, about two orders of magnitude (@fig-times): with a batch size of $1000$, cvxpylayers took $2.35~\textrm{s}$ as opposed to KINN's $0.06~\textrm{s}$.

| Metric            | Value  |
|:---------------:|:------:|
| MAE| 0.0056 p.u.|
| $R^{2}$  | 0.9972 |
: Evaluation metrics {#tbl-eval}

```{r}
#| label: fig-val
#| echo: false
#| layout-ncol: 2
#| fig-cap: "Evaluation metrics"
#| fig-subcap: 
#|      - MAE
#|      - RMSE
metrics_df <- read_csv("../Projection/projection_metrics.csv", show_col_types=FALSE)

extrafont::loadfonts(quiet = TRUE)
ggplot(metrics_df %>%select(Sample, optimality_gap), aes(x=optimality_gap,)) + geom_density(fill="grey", alpha = 0.5)   + scale_colour_tableau(
  palette = "Tableau 10", direction=1
) + scale_x_log10("Value", labels=percent_format(scale = 1)) + theme_light()
ggplot(metrics_df %>%select(Sample, inequality_violation), aes(x=inequality_violation,)) + geom_density(fill="grey", alpha = 0.5)   + scale_colour_tableau(
  palette = "Tableau 10", direction=1
) + scale_x_log10("Value") + theme_light()
```
```{r}
#| label: fig-times
#| echo: false
#| fig-cap: "Computation time comparison"
times_df <- read_csv("../Projection/times.csv", show_col_types=FALSE)

times_df <- times_df%>% pivot_longer(c(KINN, CVXPY), names_to="Method", values_to="Time")

extrafont::loadfonts(quiet = TRUE)
ggplot(times_df, aes(x=Batch_size, y=Time, color=Method)) + geom_line()   + scale_colour_tableau(
  palette = "Tableau 10", direction=-1
) + scale_y_log10() + theme_light()
```
# Conclusions

KKT-Informed Neural Network (KINN) was introduced as a neural network-based approach for solving parametric convex optimization problems. The method leverages the Karush-Kuhn-Tucker (KKT) conditions as a learning bias, integrating them into the loss function to ensure that the network’s predictions adhere to the necessary optimality criteria. This allows the network to efficiently estimate solutions while sacrificing some degree of guaranteed optimality in favor of significant improvements in computational speed and scalability.

The experimental results from the provided test case demonstrated that KINN is highly effective in providing near-optimal solutions while enabling parallel problem-solving. The comparison with traditional tools, highlighted KINN's ability to solve optimization problems in a fraction of the time, with minimal loss of accuracy. Metrics such as mean absolute error (MAE) and $R^2$ confirm that KINN produces reliable solutions within acceptable tolerances for real-world applications. Although some optimality is traded for speed, KINN provides a highly practical tool for scenarios where rapid decision-making is critical.

In future work, the potential for expanding this approach to handle non-convex optimization problems will be explored. Additionally, hybrid architectures that combine KINN with traditional optimization methods may offer further improvements in performance, providing a balance between speed and guaranteed optimality across a broader range of problem domains.